{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cf6248",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c52eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.document import Document\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "from typing import Iterable\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298a8035-7811-4165-913f-b7df27e2b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from kor.extraction import create_extraction_chain\n",
    "#from kor.nodes import Object, Text, Number\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text\n",
    "#from docx import Document\n",
    "#from pptx import Presentation\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a822f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connection import create_llm_chat_langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6263c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3b95fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=\"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\"\"uc34_back copy 2.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f276608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc2bb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour ! Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Bonjour GPT-4o !\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c444417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.resources.chat.chat.Chat at 0x23c5b15bc80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f715cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connection import create_llm_chat_langchain, create_embeddings_azureopenai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2464e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AOAI_API_VERSION = \"2024-08-01-preview\"\n",
    "AZURE_AOAI_MODEL_GPT4OMINI = \"gpt-4o\"\n",
    "AZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "llm_chat = create_llm_chat_langchain(\n",
    "    model_name = AZURE_AOAI_MODEL_GPT4OMINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599c92dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='5 + 3 equals 8.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CIwvS1qHDSf2S8ryeGjfaXsM1Mp3O', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a4628e12-9fd0-48d5-ba3e-bf071e6caee5-0', usage_metadata={'input_tokens': 15, 'output_tokens': 8, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat.invoke(\"What is 5 + 3 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498ddc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-audio\n",
      "gpt-5-nano\n",
      "gpt-audio-2025-08-28\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "o1-mini-2024-09-12\n",
      "o1-mini\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "o1-pro-2025-03-19\n",
      "o1-pro\n",
      "gpt-4o-mini-tts\n",
      "o3-2025-04-16\n",
      "o4-mini-2025-04-16\n",
      "o3\n",
      "o4-mini\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "codex-mini-latest\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "o4-mini-deep-research\n",
      "o4-mini-deep-research-2025-06-26\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1\n",
      "whisper-1\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Affiche quelques IDs\n",
    "for m in models.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c624403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp import FastMCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d508c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = create_embeddings_azureopenai(\n",
    "    model=AZURE_AOAI_MODEL_GPT4OMINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1ff65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\" You are an assistant, when i ask you about a perimeter you take it from metadata (doc_name) and \n",
    "    - when i ask you about global savings (global P&L improvement) take it from the page Global Savings Analysis\n",
    "    forget prior knowledge, answer the query.\\nQuery: {question}\\nAnswer:\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    RunnableLambda(\n",
    "        lambda x: x if isinstance(x, Iterable) else [x]\n",
    "    )   # Ensure input is iterable\n",
    "    | prompt\n",
    "    | llm_chat\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbe673",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9defa27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: x if isinstance(x, Iterable) else [x])\n",
       "| PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=' You are an assistant, when i ask you about a perimeter you take it from metadata (doc_name) and \\n    - when i ask you about global savings (global P&L improvement) take it from the page Global Savings Analysis\\n    forget prior knowledge, answer the query.\\nQuery: {question}\\nAnswer:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000014750C0AD80>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000014750C0A570>, root_client=<openai.OpenAI object at 0x0000014750BACF80>, root_async_client=<openai.AsyncOpenAI object at 0x0000014750C0AFF0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "668695f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/02 11:51:22 INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run traveling-robin-362 at: http://127.0.0.1:8080/#/experiments/446379284164137887/runs/34d9b79e3a544ae083277257817e381f\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/446379284164137887\n",
      "Run created successfully\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"test\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"foo\", \"bar\")\n",
    "    mlflow.log_metric(\"m\", 123)\n",
    "\n",
    "print(\"Run created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a230f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.metrics.genai import EvaluationExample, faithfulness\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from mlflow.metrics.genai import EvaluationExample, relevance\n",
    "\n",
    "#from func import get_rag_chain\n",
    "\n",
    "load_dotenv()  # Charge la cl√© depuis .env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. Constantes mod√®les\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# === Now you can use it exactly like OpenAI SDK ===\n",
    "AZURE_AOAI_MODEL_GPT3_TURBO = \"gpt35turbo\"\n",
    "AZURE_AOAI_MODEL_GPT4O = \"gpt-4o\"\n",
    "AZURE_AOAI_API_VERSION = \"2024-08-01-preview\"\n",
    "AZURE_AOAI_MODEL_GPT4OMINI = \"gpt-4o-mini\"\n",
    "AZURE_EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5. Faithfulness Evaluation    \n",
    "# ----------------------------------------------------------------------------\n",
    "# This example demonstrates how to use the faithfulness metric to evaluate a model's responses\n",
    "\n",
    "faithfulness_examples = [\n",
    "  EvaluationExample(\n",
    "      input=\"How much is the spend in Algeria in 2023? \",\n",
    "      output=\"Spend in Algeria is 1M‚Ç¨ in 2023.\",\n",
    "      score=5,\n",
    "      justification=\"The output provides a working solution, amount and year that is provided in the context.\",\n",
    "      grading_context = {   \n",
    "          \"context\": \"Algeria is one of the countries of the group, we noticed a new spend in 2023 of 10M‚Ç¨. with a 10% increase compared to 2022. The total spend in the group is 10M‚Ç¨ in 2023, with a 5% increase compared to 2022.\"\n",
    "      },\n",
    "  ),\n",
    "\n",
    "  EvaluationExample(\n",
    "      input=\"give th eprocurement KPIs of the group\",\n",
    "      output=\"The procurement KPIs of the group are: Spend in Algeria is 1M‚Ç¨ in 2023, Spend in France is 2M‚Ç¨ in 2023, Spend in Germany is 3M‚Ç¨ in 2023, Spend in Italy is 4M‚Ç¨ in 2023.\",\n",
    "      score=5,\n",
    "      justification=\"The output provides a working solution that is using the context provided.\",\n",
    "      grading_context={\n",
    "          \"context\": \"Algeria is one of the countries of the group, we noticed a new spend in 2023 of 10M‚Ç¨. with a 10% increase compared to 2022. The total spend in the group is 10M‚Ç¨ in 2023, with a 5% increase compared to 2022. The procurement KPIs of the group are: Spend in Algeria is 1M‚Ç¨ in 2023, Spend in France is 2M‚Ç¨ in 2023, Spend in Germany is 3M‚Ç¨ in 2023, Spend in Italy is 4M‚Ç¨ in 2023.\"\n",
    "      },\n",
    "  ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(model=AZURE_AOAI_MODEL_GPT4O, examples=faithfulness_examples)\n",
    "relevance_metric = relevance(model=AZURE_AOAI_MODEL_GPT4O)\n",
    "#print(relevance_metric)\n",
    "#print(faithfulness_metric)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. DataFrame d'√©valuation\n",
    "# ----------------------------------------------------------------------------\n",
    "# Create a DataFrame for evaluation questions\n",
    "eval_df = pd.DataFrame(\n",
    "{\n",
    "    \"questions\": [\n",
    "        \"how much spend in france?\",\n",
    "        \"provides me with spend in algeria?\",\n",
    "        \"give me procurement kpis of italy?\",\n",
    "    ],\n",
    "        # Colonne de r√©f√©rence (targets) n√©cessaire pour l'√©valuation\n",
    "    \"ground_truth\": [\n",
    "        \"Spend in France is 2M‚Ç¨ in 2023.\",\n",
    "        \"Spend in Algeria is 10M‚Ç¨ in 2023.\",\n",
    "        \"Procurement KPIs of Italy are: Spend in Italy is 4M‚Ç¨ in 2023.\"\n",
    "    ]\n",
    "}\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4. Fonction de mod√®le\n",
    "# ----------------------------------------------------------------------------\n",
    "def model(input_df: pd.DataFrame) -> list[str]:\n",
    "    \"\"\"Pour chaque question, on fait la retrieval + answer via RAG.\"\"\"\n",
    "    outputs = []\n",
    "    for _, row in input_df.iterrows():\n",
    "        q = row[\"questions\"]\n",
    "        result = rag_chain.invoke({\"question\": q})\n",
    "        outputs.append(result)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c02f1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(lambda x: x if isinstance(x, Iterable) else [x])\n",
       "| PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template=' You are an assistant, when i ask you about a perimeter you take it from metadata (doc_name) and \\n    - when i ask you about global savings (global P&L improvement) take it from the page Global Savings Analysis\\n    forget prior knowledge, answer the query.\\nQuery: {question}\\nAnswer:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000014750C0AD80>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000014750C0A570>, root_client=<openai.OpenAI object at 0x0000014750BACF80>, root_async_client=<openai.AsyncOpenAI object at 0x0000014750C0AFF0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de0bb0e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableSequence' object has no attribute 'retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m eval_df[\u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43meval_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(q)\u001b[39m\n\u001b[32m      1\u001b[39m eval_df[\u001b[33m\"\u001b[39m\u001b[33msource_documents\u001b[39m\u001b[33m\"\u001b[39m] = eval_df[\u001b[33m\"\u001b[39m\u001b[33mquestions\u001b[39m\u001b[33m\"\u001b[39m].apply(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m q: \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretriever\u001b[49m.get_relevant_documents(q)\n\u001b[32m      3\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\pydantic\\main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'RunnableSequence' object has no attribute 'retriever'"
     ]
    }
   ],
   "source": [
    "eval_df[\"source_documents\"] = eval_df[\"questions\"].apply(\n",
    "    lambda q: rag_chain.retriever.get_relevant_documents(q)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/02 14:28:38 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run dapper-mink-418 at: http://127.0.0.1:8080/#/experiments/446379284164137887/runs/7030c518b9fb4920bf633e2d196cb87f\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/446379284164137887\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Error: Metric calculation failed for the following metrics:\nMetric 'faithfulness' requires the following:\n- missing columns ['context'] need to be defined or mapped\nMetric 'relevance' requires the following:\n- missing columns ['context'] need to be defined or mapped\n\nBelow are the existing column names for the input/output data:\nInput Columns: ['questions', 'ground_truth']\nOutput Columns: ['result']\n\nTo resolve this issue, you may need to:\n- specify any required parameters\n- if you are missing columns, check that there are no circular dependencies among your\nmetrics, and you may want to map them to an existing column using the following\nconfiguration:\nevaluator_config={'col_mapping': {<missing column name>: <existing column name>}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 6. Model Evaluation\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion-answering\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresult\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcol_mapping\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(results.metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\deprecated.py:23\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_databricks_uri(tracking_uri):\n\u001b[32m     13\u001b[39m     warnings.warn(\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use these new alternatives:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m     22\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\base.py:1790\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, extra_metrics, custom_artifacts, env_manager, model_config, inference_params, model_id, _called_from_genai_evaluate)\u001b[39m\n\u001b[32m   1787\u001b[39m predictions_expected_in_model_output = predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     evaluate_result = \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\base.py:1031\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(model, model_type, model_id, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, extra_metrics, custom_artifacts, predictions, evaluators)\u001b[39m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_.evaluator.can_evaluate(model_type=model_type, evaluator_config=eval_.config):\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m configure_autologging_for_evaluation(enable_tracing=should_enable_tracing):\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m         eval_result = \u001b[43meval_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m eval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1044\u001b[39m         eval_results.append(eval_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\default_evaluator.py:947\u001b[39m, in \u001b[36mBuiltInEvaluator.evaluate\u001b[39m\u001b[34m(self, model_type, dataset, run_id, evaluator_config, model, extra_metrics, custom_artifacts, predictions, model_id, **kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m TempDir() \u001b[38;5;28;01mas\u001b[39;00m temp_dir, matplotlib.rc_context(_matplotlib_config):\n\u001b[32m    946\u001b[39m     \u001b[38;5;28mself\u001b[39m.temp_dir = temp_dir\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\evaluators\\default.py:77\u001b[39m, in \u001b[36mDefaultEvaluator._evaluate\u001b[39m\u001b[34m(self, model, extra_metrics, custom_artifacts, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m y_true = \u001b[38;5;28mself\u001b[39m.dataset.labels_data\n\u001b[32m     76\u001b[39m metrics = \u001b[38;5;28mself\u001b[39m._builtin_metrics() + extra_metrics\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mother_model_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluate_and_log_custom_artifacts(custom_artifacts, prediction=y_pred, target=y_true)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Log metrics and artifacts\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\default_evaluator.py:753\u001b[39m, in \u001b[36mBuiltInEvaluator.evaluate_metrics\u001b[39m\u001b[34m(self, metrics, prediction, target, other_output_df)\u001b[39m\n\u001b[32m    749\u001b[39m eval_df = \u001b[38;5;28mself\u001b[39m._get_eval_df(prediction, target)\n\u001b[32m    750\u001b[39m metrics = [\n\u001b[32m    751\u001b[39m     MetricDefinition.from_index_and_metric(i, metric) \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics)\n\u001b[32m    752\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_order_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[38;5;28mself\u001b[39m._test_first_row(metrics, eval_df, other_output_df)\n\u001b[32m    757\u001b[39m \u001b[38;5;66;03m# calculate metrics for the full eval_df\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\default_evaluator.py:689\u001b[39m, in \u001b[36mBuiltInEvaluator._order_metrics\u001b[39m\u001b[34m(self, metrics, eval_df, other_output_df)\u001b[39m\n\u001b[32m    687\u001b[39m     \u001b[38;5;66;03m# cant calculate any more metrics\u001b[39;00m\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m did_append_metric:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_exception_for_malformed_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfailed_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_output_df\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m     remaining_metrics = pending_metrics\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ordered_metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SCHENNOUFI\\AppData\\Local\\anaconda3\\envs\\genai\\Lib\\site-packages\\mlflow\\models\\evaluation\\default_evaluator.py:643\u001b[39m, in \u001b[36mBuiltInEvaluator._raise_exception_for_malformed_metrics\u001b[39m\u001b[34m(self, malformed_results, eval_df, other_output_df)\u001b[39m\n\u001b[32m    637\u001b[39m         input_columns.append(\u001b[33m\"\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    639\u001b[39m error_message = \u001b[38;5;28mself\u001b[39m._construct_error_message_for_malformed_metrics(\n\u001b[32m    640\u001b[39m     malformed_results, input_columns, output_columns\n\u001b[32m    641\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(error_message, error_code=INVALID_PARAMETER_VALUE)\n",
      "\u001b[31mMlflowException\u001b[39m: Error: Metric calculation failed for the following metrics:\nMetric 'faithfulness' requires the following:\n- missing columns ['context'] need to be defined or mapped\nMetric 'relevance' requires the following:\n- missing columns ['context'] need to be defined or mapped\n\nBelow are the existing column names for the input/output data:\nInput Columns: ['questions', 'ground_truth']\nOutput Columns: ['result']\n\nTo resolve this issue, you may need to:\n- specify any required parameters\n- if you are missing columns, check that there are no circular dependencies among your\nmetrics, and you may want to map them to an existing column using the following\nconfiguration:\nevaluator_config={'col_mapping': {<missing column name>: <existing column name>}}"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:8080/static-files/lib/notebook-trace-renderer/index.html?trace_id=03ee287c28c14846a1f3c6a6c2d4f8f5&amp;experiment_id=446379284164137887&amp;trace_id=32865a16c67341f6bc9ce0a259f08738&amp;experiment_id=446379284164137887&amp;trace_id=70a520ad87ba47b3823622b3498ab84b&amp;experiment_id=446379284164137887&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=03ee287c28c14846a1f3c6a6c2d4f8f5), Trace(trace_id=32865a16c67341f6bc9ce0a259f08738), Trace(trace_id=70a520ad87ba47b3823622b3498ab84b)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# 6. Model Evaluation\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "results = mlflow.evaluate(\n",
    "model,\n",
    "eval_df,\n",
    "model_type=\"question-answering\",\n",
    "evaluators=\"default\",\n",
    "predictions=\"result\",\n",
    "targets = \"ground_truth\",\n",
    "extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n",
    "evaluator_config={\n",
    "    \"col_mapping\": {\n",
    "        \"inputs\": \"questions\",\n",
    "        \"context\": \"source_documents\",\n",
    "        \"targets\": \"ground_truth\",\n",
    "    }\n",
    "},\n",
    ")\n",
    "print(results.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8db57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa5492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744004b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671987e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Formatage du document\n",
    "def format_docs(docs: Iterable[LCDocument]):\n",
    "    print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# ‚úÖ Extraction dynamique des filtres √† partir de la requ√™te\n",
    "\n",
    "#Alias utilisateur vers p√©rim√®tre officiel\n",
    "alias_perimeters = {\n",
    "    \"group functions\": \"Group Functions (excl. ITG & IMEX)\",\n",
    "    \"Group Functions\": \"Group Functions (excl. ITG & IMEX)\",\n",
    "    \"bnp2i\": \"BNP Paribas Partners for Innovation\",\n",
    "    \"bp2i\": \"BNP Paribas Partners for Innovation\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Liste des p√©rim√®tres valides\n",
    "valid_perimeters = [\n",
    "    \"Switzerland\", \"Portugal\", \"United Kingdom\", \"Italy\", \"Poland\", \"NAR\",\n",
    "    \"Luxembourg\", \"APAC\", \"Germany & Austria\", \"Belgium\", \"Arval\",\n",
    "    \"Asset Management\", \"BNP Paribas Personal Finance\", \"Wealth Management\",\n",
    "    \"Real Estate\", \"Leasing Solutions\", \"BNP Paribas Partners for Innovation\",\n",
    "    \"ITG\", \"Insurance\", \"Commercial, Personal Bank in France\",\n",
    "    \"Group Functions (excl. ITG & IMEX)\", \"CIB\", \"FLOA BANK\"\n",
    "]\n",
    "\n",
    "def extract_filters_from_query(user_query):\n",
    "    filters = {}\n",
    "    extracted_perimeter = None\n",
    "    normalized_query = user_query.lower()\n",
    "    \n",
    "    # 1. Extraction du p√©rim√®tre (jusqu‚Äô√† 4 mots max)\n",
    "    match = re.search(r\"(?:p[√©e]rim√®tre|perimeter)\\s*[:\\-]?\\s*((?:\\w+[\\s&]*){1,10})\", user_query, re.IGNORECASE)\n",
    "    if not match:\n",
    "        # Si pas trouv√© via \"p√©rim√®tre\", on essaie \"pour\", \"for\", \"of\"\n",
    "        match = re.search(r\"\\b(?:pour|for|of)\\s+((?:\\w+[\\s&]*){1,4})\", user_query, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        perimeter = match.group(1).strip()\n",
    "        # Nettoyage : on coupe √† partir de certains mots cl√©s (is, for, etc.)\n",
    "        perimeter = re.split(r\"\\b(?:is|are|on|with|and)\\b\", perimeter, flags=re.IGNORECASE)[0].strip()\n",
    "\n",
    "        # V√©rifie d'abord si un alias exact est utilis√©\n",
    "        if perimeter.lower() in alias_perimeters:\n",
    "            filters[\"perimeter\"] = alias_perimeters[perimeter.lower()]\n",
    "        else:\n",
    "            # Sinon, correspondance partielle classique\n",
    "            for vp in valid_perimeters:\n",
    "                if perimeter.lower() in vp.lower():\n",
    "                    filters[\"perimeter\"] = vp\n",
    "                    break\n",
    "            else:\n",
    "                extracted_perimeter = perimeter  # non reconnu, mais on continue\n",
    "\n",
    "    # 2. D√©tection du type d‚Äôanalyse et de la page\n",
    "    if \"kpi\" in normalized_query and \"spend\" in normalized_query:\n",
    "        filters[\"Analyse\"] = \"spend\"\n",
    "        filters[\"page_title\"] = \"Executive Summary\"\n",
    "\n",
    "    elif \"kpi\" in normalized_query and \"saving\" in normalized_query:\n",
    "        filters[\"Analyse\"] = \"savings\"\n",
    "        filters[\"page_number\"] = 1\n",
    "\n",
    "    elif any(term in normalized_query for term in [\"p&l improvement\", \"cost avoidance\", \"savings on project\"]):\n",
    "        filters[\"Analyse\"] = \"savings\"\n",
    "        filters[\"page_number\"] = 1\n",
    "\n",
    "    elif \"spend\" in normalized_query:\n",
    "        filters[\"Analyse\"] = \"spend\"\n",
    "        filters[\"page_title\"] = \"Executive Summary\"\n",
    "\n",
    "    elif \"saving\" in normalized_query:\n",
    "        filters[\"Analyse\"] = \"savings\"\n",
    "        filters[\"page_number\"] = 1\n",
    "    elif \"saving\" in normalized_query and \"family\" in normalized_query:\n",
    "        filters[\"Analyse\"] = \"saving\"\n",
    "        filters[\"page_number\"] = 1\n",
    "\n",
    "    return filters\n",
    "# ‚úÖ Conversion vers filtre Chroma\n",
    "\n",
    "def convert_to_chroma_filter(filters: dict):\n",
    "    if not filters:\n",
    "        return None\n",
    "    if len(filters) == 1:\n",
    "        return filters\n",
    "    return {\"$and\": [{k: v} for k, v in filters.items()]}\n",
    "\n",
    "# ‚úÖ R√©cup√©ration du contexte\n",
    "\n",
    "def get_context_with_filters(inputs):\n",
    "    user_query = inputs[\"question\"] if isinstance(inputs, dict) else inputs\n",
    "    filters = extract_filters_from_query(user_query)\n",
    "    chroma_filter = convert_to_chroma_filter(filters)\n",
    "    retriever = db.as_retriever(search_kwargs={\"filter\": chroma_filter}) if chroma_filter else db.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(user_query)\n",
    "    return {\n",
    "        \"context\": format_docs(docs),\n",
    "        \"question\": user_query,\n",
    "        \"chat_history\": inputs.get(\"chat_history\", \"\") if isinstance(inputs, dict) else \"\"\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf265f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ RAG Chain\n",
    "get_filtered_context = RunnableLambda(get_context_with_filters)\n",
    "\n",
    "rag_chain = (\n",
    "    get_filtered_context\n",
    "    | prompt\n",
    "    | llm_chat\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0059e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = r\"C:\\Users\\SCHENNOUFI\\OneDrive - Micropole\\Bureau\\WorkSpace\\IA GEN\\CHATBOT\\prod\\chromadb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc46efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Initialisation externe\n",
    "\n",
    "def initialize_chain(llm_instance, embeddings_instance):\n",
    "    global db, llm_chat\n",
    "    llm_chat = llm_instance\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144057b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b6076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a40998e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd1f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "576b7512",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
